# Datasets

This directory provides our datasets of Dockerfiles. We have included the datasets in each of the (many) representations they pass through prior to rule mining. In doing so, we hope that others can expand, improve, revise, and compare this rule mining pipeline.

Each sub-folder will have a `github.*` file and a, substantially smaller, `gold.*` file. The `github.*` files correspond to our set of (~250,000 original, ~180,000 deduplicated) Dockerfiles sourced from GitHub. The `gold.*` files correspond to the subset of the GitHub data that originated in the `docker-library/` GitHub organization. The `docker-library/` organization is run by Docker and contains file of, presumably, high quality.

## (0a) `datasets/0a-original-dockerfile-sources`

This representation has the original Dockerfiles at the source level. These were downloaded directly from GitHub and, at this stage, have had no processing of any kind applied. They were selected using a, very permissive, file-name-based filter: `*.(D|d)ockerfile.*` .

### Example Usage

**Representation:** this dataset is an archive file (format: `.tar.xz`) that, when extracted, yields a directory with each Dockerfile's original source (as text) saved into a file with a name based on the unique ID GitHub assigned to that file. (Unique ID's were extracted from GitHub's API.)

**Example:** 
```bash
tar -xvJf ./datasets/0a-original-dockerfile-sources/github.tar.xz
cat ./sources/484097305.Dockerfile
```
```Dockerfile
FROM busybox
EXPOSE 80/tcp
COPY httpserver .
CMD ["./httpserver"]
```

### Dataset Statistics

Size:
```
github.tar.xz: 20M compressed/897M uncompressed
gold.tar.xz: 64K compressed/2.2M uncompressed
```

Count:
```
github.tar.xz: 219061 files
gold.tar.xz: 432 files
```

Verify count with:
```bash
# github
tar -tJf ./0a-original-dockerfile-sources/github.tar.xz \
  | grep '.Dockerfile' \
  | wc -l
# gold
tar -tJf ./0a-original-dockerfile-sources/gold.tar.xz \
  | grep '.Dockerfile' \
  | wc -l
```

Hashes:
```
github.tar.xz (uncompressed contents):
2bcd4448c587d0afac9b7dfbd84897326eec86ae728317c17d9404aeef3c4d05

gold.tar.xz (uncompressed contents):
45decb7a78f44f3ba8210918141c74a38541e686b451a583029f1dfe959a40a6
```

Verify hash with:
```bash
tar -xJOf ./0a-original-dockerfile-sources/github.tar.xz \
  | sha256sum | awk '{ print $1 }'
tar -xJOf ./0a-original-dockerfile-sources/gold.tar.xz \
  | sha256sum | awk '{ print $1 }'
```

## (0b) `datasets/0b-deduplicated-dockerfile-sources`

Here we have the Dockerfiles that are unique based on a SHA256 hash of their contents. (A small area of improvement may be removing comments prior to doing deduplication.)

### Example Usage

**Representation:** this dataset is an archive file (format: `.tar.xz`) that, when extracted, yields a directory where each Dockerfile and has a filename based on the unique ID GitHub assigned to that file.

**Example:** 
```bash
tar -xvJf ./0b-deduplicated-dockerfile-sources/dataset.tar.xz
cat ./deduplicated-sources/291348084.Dockerfile 
```
```Dockerfile
# https://hub.docker.com/r/consensysllc/go-ipfs/
# THANKS!!!!!

FROM ipfs/go-ipfs
COPY start_ipfs.sh /usr/local/bin/start_ipfs
```

### Dataset Statistics

Size: `29M compressed/741M uncompressed`.

Count: `178506 files`.

Verify count with:
```bash
tar -tJf ./0b-deduplicated-dockerfile-sources/dataset.tar.xz \
  | grep '.Dockerfile' \
  | wc -l
# Or/also
cat ./5-dockerfile-metadata/dataset.jsonl.xz \
  | xz -cd \
  | jq '.file_sha' \
  | sort -u \
  | wc -l
```

Hash: `de1b5c0cd0bb318a093a9efb19d446c755b1a4bf29f4649625a7f197c9f29fa6`

Verify hash with:
```bash
tar -xJOf ./0b-deduplicated-dockerfile-sources/dataset.tar.xz \
  | sha256sum | awk '{ print $1 }'
```

### Regenerate from representation (0a)

This is just an example (you may need to slightly modify this to work in your environment). This assumes, in your current directory, you've extracted `./0a-original-dockerfile-sources/github.tar.xz`:

```bash
# Mkdir to hold deduplicated sources
mkdir -p ./deduplicated-sources

# Deduplicate (using both representation 0a and 5-dockerfile-metadata)
while read p; do
  cat ./sources/$(echo "${p}" | jq -r .file_id).Dockerfile \
    > ./deduplicated-sources/$(echo "${p}" | jq -r .file_sha).Dockerfile;
done < <(
  cat ./5-dockerfile-metadata/github.jsonl.xz | xz -cd
)

# Compress
tar -cJvf ./0b-deduplicated-dockerfile-sources/github.tar.xz \
  ./deduplicated-sources/
```

## (1) `datasets/1-phase-1-dockerfile-asts`

Here, for each Dockerfile in the previous representation (one-to-one), we have an AST generated by a Python based Dockerfile parser. These ASTs are quite shallow and, unfortunately, not the best target for further analysis.

### Example Usage

**Representation:** this dataset is a compressed (format: `jsonl.xz`) JSON lines file. Each Phase-I AST is stored as a JSON object (one per line).

**Example:** 
```bash
cat ./1-phase-1-dockerfile-asts/dataset.jsonl.xz \
  | xz -cd \
  | grep '3d0d691c1745e14be0f1facd14c49e3fbbb750d8' \
  | jq
```

```json
{
  "type": "DOCKER-FILE",
  "children": [
    {
      "type": "DOCKER-FROM",
      "children": [
        {
          "type": "DOCKER-IMAGE-NAME",
          "value": "solaris",
          "children": []
        }
      ]
    },
    ...,
    {
      "type": "DOCKER-CMD",
      "children": [
        {
          "type": "DOCKER-CMD-ARG",
          "value": "./httpserver",
          "children": []
        }
      ]
    }
  ],
  "file_sha": "3d0d691c1745e14be0f1facd14c49e3fbbb750d8"
}
```

### Dataset Statistics

Size: `24M compressed/338M uncompressed`.

Count: `178452 JSON objects`.

Verify count with:
```bash
cat ./1-phase-1-dockerfile-asts/dataset.jsonl.xz | xz -cd | wc -l
```

Hash: `bed9d05e385e18f823c28f3dc02b3b03a2d9af8d1e1fd8c8786acbb55087a81c`

Verify hash with:
```bash
cat ./1-phase-1-dockerfile-asts/dataset.jsonl.xz | xz -cd \
  | sha256sum | awk '{ print $1 }'
```

### Re-generate from representation (0b)

To regenerate from deduplicated Dockerfile sources use: `./1-phase-1-dockerfile-asts/generate.sh` or (if not using a *nix system) manually build and run the Dockerfile in the `./1-phase-1-dockerfile-asts/generate/` directory. (If this second approach is used, one must volume mount `./0b-deduplicated-dockerfile-sources` to `/mnt/inputs` and `./1-phase-1-dockerfile-asts` to `/mnt/outputs`.)

## (2) `datasets/2-phase-2-dockerfile-asts`

In this representation we again have, for each Dockerfile AST from the previous step (one-to-one), a new AST in which embedded shell has been parsed using the [ShellCheck](https://github.com/koalaman/shellcheck) tool. This representation is richer and is comparable to the representation used in most recent Dockerfile studies have used.

(Dockerfiles need not embed _Bash_, so there is the possibility of parse failures that could be resolved by using, as one example, a PowerShell parser. Heuristically identifying the kind of embedded shell used and parsing more than Bash is an interesting opportunity for future work.)

### Example Usage

**Representation:** this dataset is a compressed (format: `jsonl.xz`) JSON lines file. Each Phase-I AST is stored as a JSON object (one per line).

**Example:** 
```bash
cat ./2-phase-2-dockerfile-asts/dataset.jsonl.xz \
  | xz -cd \
  | grep '972b56dc14ff87faddd0c35a5f3b6a32597a36ed' \
  | jq
```

```json
{
  "children": [
    {
      "children": [
        {
          "value": "node",
          "children": [],
          "type": "DOCKER-IMAGE-NAME"
        },
        {
          "value": "10",
          "children": [],
          "type": "DOCKER-IMAGE-TAG"
        }
      ],
      "type": "DOCKER-FROM"
    },
    ...,
    {
      "children": [
        {
          "children": [
            {
              "children": [
                {
                  "children": [],
                  "type": "BASH-COMMAND-PREFIX"
                },
                {
                  "children": [
                    {
                      "value": "npm",
                      "children": [],
                      "type": "BASH-LITERAL"
                    }
                  ],
                  "type": "BASH-COMMAND-COMMAND"
                },
                {
                  "children": [
                    {
                      "value": "install",
                      "children": [],
                      "type": "BASH-LITERAL"
                    },
                    {
                      "value": "--production",
                      "children": [],
                      "type": "BASH-LITERAL"
                    }
                  ],
                  "type": "BASH-COMMAND-ARGS"
                }
              ],
              "type": "MAYBE-SEMANTIC-COMMAND"
            }
          ],
          "type": "BASH-SCRIPT"
        }
      ],
      "type": "DOCKER-RUN"
    },
    ...,
    {
      "children": [
        {
          "value": "bin/cncjs",
          "children": [],
          "type": "DOCKER-CMD-ARG"
        }
      ],
      "type": "DOCKER-CMD"
    }
  ],
  "type": "DOCKER-FILE",
  "file_sha": "972b56dc14ff87faddd0c35a5f3b6a32597a36ed"
}

```

### Dataset Statistics

Size: `36M compressed/1.2G uncompressed`.

Count: `178452 JSON objects`.

Verify count with:
```bash
cat ./2-phase-2-dockerfile-asts/dataset.jsonl.xz | xz -cd | wc -l
```

Hash: `d88b85d8e705f9720dc031a7a1b2db94c60fce097b322e167d3a658e45d48e7c`

Verify hash with:
```bash
cat ./2-phase-2-dockerfile-asts/dataset.jsonl.xz | xz -cd \
  | sha256sum | awk '{ print $1 }'
```

### Re-generate from representation (1)

To regenerate from Phase-I ASTs use: `./2-phase-2-dockerfile-asts/generate.sh` or (if not using a *nix system) manually build and run the Dockerfile in the `./2-phase-2-dockerfile-asts/generate/` directory. (If this second approach is used, one must volume mount `./1-phase-1-dockerfile-asts` to `/mnt/inputs` and `./2-phase-2-dockerfile-asts` to `/mnt/outputs`.)

## (3) `datasets/3-phase-3-dockerfile-asts`

In this penultimate representation we have another (one-to-one) mapping from ASTs in the previous step to new ASTs. The difference in this new representation is that yet another level of parsing has been applied (using 50 generated parsers). Details on this Phase III representation and its utility are one of the primary facets of our paper.


### Dataset Statistics

Size: `26M compressed/828M uncompressed`.

Count: `178452 JSON objects`.

Verify count with:
```bash
cat ./3-phase-3-dockerfile-asts/dataset.jsonl.xz | xz -cd | wc -l
```

Hash: `ab4944a937de15710261ea83d04660755700375e0847fb07aaa31746603f1d67`

Verify hash with:
```bash
cat ./3-phase-3-dockerfile-asts/dataset.jsonl.xz | xz -cd \
  | sha256sum | awk '{ print $1 }'
```

### Re-generate from representation (2)

To regenerate from Phase-II ASTs use: `./3-phase-3-dockerfile-asts/generate.sh` or (if not using a *nix system) manually build and run the Dockerfile in the `./3-phase-3-dockerfile-asts/generate/` directory. (If this second approach is used, one must volume mount `./2-phase-2-dockerfile-asts` to `/mnt/inputs` and `./3-phase-3-dockerfile-asts` to `/mnt/outputs`.)


## (4) `datasets/4-abstracted-asts`

Finally, for each enriched AST from the last representation (one-to-one), we apply a configurable group of Regex based abstractions to literal values. This gives us new ASTs, in the same form, with more "abstract" nodes that may be of value during pattern mining. Indeed, this is the representation of Dockerfiles we use for mining our patterns.

### Example Usage

### Dataset Statistics

## (5) `datasets/5-dockerfile-metadata`

This is an extra dataset (**not** derived from the previously listed representation) that contains JSON objects (one per line) with information on each of the Dockerfiles we used. An example record is shown below:

```json
{
  "file_contents_scraped_at": "2019-06-24T22:10:06",
  "file_directory": "",
  "file_id": 133495483,
  "file_name": "Dockerfile",
  "file_sha": "a2f4e76c9a16dbdaecf623f2878dd66b9609c371",
  "file_url": "https://github.com/dordnung/System2/blob/master/Dockerfile",
  "repo_branch": "master",
  "repo_full_name": "dordnung/System2",
  "repo_id": 7636623
}
```

### Example Usage

### Dataset Statistics

Size: `9.3M compressed/83M uncompressed`.

Count: `178506 JSON objects`.

Verify count with:
```bash
cat ./5-dockerfile-metadata/dataset.jsonl.xz | xz -cd | wc -l
```

---

## Notes

In general, when we map files from one representation to the next in a one-to-one way there is the possibility of failures (for various reasons: one example may be a Dockerfile that has a malformed RUN statement that isn't malformed according to the simple syntax of Dockerfiles but is according to ShellChecks more refined understanding of Bash). Failures such as these will give us less data points in one representation than we had in the previous (even though, theoretically, we'd have a one-to-one correspondence).
